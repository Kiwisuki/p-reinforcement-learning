{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import torch as th\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "\n",
    "from game_2048 import Game2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DISCRETE_ACTIONS = 4\n",
    "OBS_SPACE_LIM = [[8193 for _ in range(4)] for _ in range(4)]\n",
    "LOGDIR = \"./tensorboards\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "    def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            self.game = Game2048()\n",
    "            self.action_space = spaces.Discrete(4)\n",
    "            self.observation_space = spaces.Box(low=-1, high=1, shape=(88,), dtype=np.float32)\n",
    "\n",
    "            self.last_state = np.array(self.game.board)\n",
    "            self.last_states = [copy.deepcopy(self.last_state) for _ in range(10)]\n",
    "            self.total_steps = 0\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        current_state, reward_increment, done = self.game.move(action)\n",
    "        current_state = np.array(current_state)\n",
    "        self.update_memory(current_state)\n",
    "        self.total_steps += 1\n",
    "\n",
    "\n",
    "        future_outcomes = [\n",
    "                        Game2048.left(current_state)[0], \n",
    "                        Game2048.right(current_state)[0],\n",
    "                        Game2048.up(current_state)[0], \n",
    "                        Game2048.down(current_state)[0]\n",
    "                        ]\n",
    "        \n",
    "        future_rewards = [\n",
    "                        Game2048.left(current_state)[1],\n",
    "                        Game2048.right(current_state)[1],\n",
    "                        Game2048.up(current_state)[1],\n",
    "                        Game2048.down(current_state)[1]\n",
    "                        ]\n",
    "        \n",
    "        stuck_future_outcomes = [np.array_equal(current_state, future_outcomes[i]) * 1 for i in range(4)]\n",
    "\n",
    "\n",
    "        reward = self.count_empty_cells(current_state) - self.count_empty_cells(self.last_states[-2])\n",
    "\n",
    "        observation = self.scale_observation(future_outcomes, stuck_future_outcomes, current_state, future_rewards)\n",
    "\n",
    "        done =  np.array_equal(stuck_future_outcomes, np.ones(4))\n",
    "        truncated =  all(np.array_equal(state, current_state) for state in self.last_states)\n",
    "\n",
    "        if 2048 in current_state:\n",
    "            reward += 1000\n",
    "            done = True\n",
    "\n",
    "        info = {}\n",
    "        return observation, reward, done, truncated, info\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.game = Game2048(seed)\n",
    "        else:\n",
    "            self.game = Game2048()\n",
    "\n",
    "        self.last_state = np.array(self.game.board)\n",
    "        self.last_states = [copy.deepcopy(self.last_state) for _ in range(10)]\n",
    "        current_state = np.array(self.game.board)\n",
    "        self.update_memory(current_state)\n",
    "\n",
    "        future_outcomes = [\n",
    "            Game2048.left(current_state)[0],\n",
    "            Game2048.right(current_state)[0],\n",
    "            Game2048.up(current_state)[0],\n",
    "            Game2048.down(current_state)[0]\n",
    "        ]\n",
    "\n",
    "        future_rewards = [\n",
    "            Game2048.left(current_state)[1],\n",
    "            Game2048.right(current_state)[1],\n",
    "            Game2048.up(current_state)[1],\n",
    "            Game2048.down(current_state)[1]\n",
    "        ]\n",
    "\n",
    "        stuck_future_outcomes = [np.array_equal(current_state, future_outcomes[i]) * 1 for i in range(4)]\n",
    "\n",
    "        observation = self.scale_observation(future_outcomes, stuck_future_outcomes, current_state, future_rewards)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        self.game.render_board(self.game.board)\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def count_empty_cells(board):\n",
    "        return np.count_nonzero(np.array(board) == 0)\n",
    "\n",
    "    def valid_action_mask(self):\n",
    "        if 0 in self.game.board:\n",
    "            return np.ones(4).astype(np.float32)\n",
    "        \n",
    "        valid_left = Game2048.left(self.game.board)[0] != self.game.board\n",
    "        valid_right = Game2048.right(self.game.board)[0] != self.game.board\n",
    "        valid_up = Game2048.up(self.game.board)[0] != self.game.board\n",
    "        valid_down = Game2048.down(self.game.board)[0] != self.game.board\n",
    "        return np.array([valid_left, valid_right, valid_up, valid_down]).astype(np.float32)\n",
    "    \n",
    "    def update_memory(self, new_state):\n",
    "        self.last_states.pop(0)\n",
    "        self.last_states.append(copy.deepcopy(new_state))\n",
    "\n",
    "\n",
    "    def scale_observation(self, future_outcomes, stuck_future_outcomes, current_state, future_rewards):\n",
    "        future_outcomes = np.array(future_outcomes)\n",
    "        stuck_future_outcomes = np.array(stuck_future_outcomes)\n",
    "        current_state = np.array(current_state)\n",
    "        future_rewards = np.array(future_rewards)\n",
    "\n",
    "        current_state = (current_state - 1024) / 1024\n",
    "        future_rewards = (future_rewards - 1024) / 1024\n",
    "\n",
    "        for i, outcome in enumerate(future_outcomes):\n",
    "            outcome = (outcome - 1024) / 1024\n",
    "            future_outcomes[i] = outcome\n",
    "\n",
    "        observation = np.concatenate([current_state.ravel(), np.concatenate(future_outcomes).ravel(),\n",
    "                        np.array(stuck_future_outcomes).astype(np.float32), future_rewards]).astype(np.float32)\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = GameEnv()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, num_steps=100):\n",
    "    scores = []\n",
    "    boards = []\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        env = GameEnv()\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        rewards = []\n",
    "        while not done:\n",
    "            action, info = model.predict(obs, deterministic=True)\n",
    "            action = int(model.predict(obs, deterministic=True)[0])\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            done = done or truncated\n",
    "            rewards.append(reward)\n",
    "        boards.append(env.game.board)\n",
    "        scores.append(env.game.total_score)\n",
    "\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    print(f\"Average score: {scores.mean()}\")\n",
    "\n",
    "    best_board = boards[np.argmax(scores)]\n",
    "    # print max score\n",
    "    print(f'Max score: {np.max(scores)}')\n",
    "    env.game.render_board(best_board)\n",
    "\n",
    "# Random baseline is ~1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariusarlauskas/anaconda3/envs/surf_rl/lib/python3.9/site-packages/stable_baselines3/common/save_util.py:278: UserWarning: Path 'models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 5024.168\n",
      "Max score: 15312\n",
      "+-------------------+\n",
      "| 8  | 2  | 8  | 2  |\n",
      "+-------------------+\n",
      "|1024| 32 | 2  | 4  |\n",
      "+-------------------+\n",
      "|256 | 16 | 64 | 8  |\n",
      "+-------------------+\n",
      "| 2  |512 | 8  | 4  |\n",
      "+-------------------+\n",
      "Average score: 5194.404\n",
      "Max score: 14704\n",
      "+-------------------+\n",
      "| 4  | 8  | 4  | 2  |\n",
      "+-------------------+\n",
      "| 8  | 16 | 32 | 4  |\n",
      "+-------------------+\n",
      "| 4  |128 |1024| 2  |\n",
      "+-------------------+\n",
      "|128 |512 | 2  | 4  |\n",
      "+-------------------+\n",
      "Average score: 5384.308\n",
      "Max score: 16640\n",
      "+-------------------+\n",
      "| 2  | 4  | 16 |256 |\n",
      "+-------------------+\n",
      "| 4  | 32 |128 |1024|\n",
      "+-------------------+\n",
      "| 8  | 16 | 32 |512 |\n",
      "+-------------------+\n",
      "| 2  | 4  | 16 |128 |\n",
      "+-------------------+\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# MaskablePPO behaves the same as SB3's PPO unless the env is wrapped\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# with ActionMasker. If the wrapper is detected, the masks are automatically\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# retrieved and used when learning. Note that MaskablePPO does not accept\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m#policy_kwargs = dict(activation_fn=th.nn.ReLU,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m#                        net_arch=dict(pi=[84*2, 84*3, 84, 10], vf=[84*2, 84*3, 84, 10]))\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model \u001b[39m=\u001b[39m MaskablePPO(MaskableActorCriticPolicy, env, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, tensorboard_log\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./tensorboards_maskable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1_000_000\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m1.5\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m6\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels/2048_maskable_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m evaluate(model, num_steps\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/surf_rl/lib/python3.9/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:526\u001b[0m, in \u001b[0;36mMaskablePPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001b[0m\n\u001b[1;32m    523\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    525\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 526\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps, use_masking)\n\u001b[1;32m    528\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/surf_rl/lib/python3.9/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:303\u001b[0m, in \u001b[0;36mMaskablePPO.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps, use_masking)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[39mif\u001b[39;00m use_masking:\n\u001b[1;32m    301\u001b[0m         action_masks \u001b[39m=\u001b[39m get_action_masks(env)\n\u001b[0;32m--> 303\u001b[0m     actions, values, log_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(obs_tensor, action_masks\u001b[39m=\u001b[39;49maction_masks)\n\u001b[1;32m    305\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    306\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n",
      "File \u001b[0;32m~/anaconda3/envs/surf_rl/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/surf_rl/lib/python3.9/site-packages/sb3_contrib/common/maskable/policies.py:139\u001b[0m, in \u001b[0;36mMaskableActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic, action_masks)\u001b[0m\n\u001b[1;32m    137\u001b[0m distribution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m action_masks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     distribution\u001b[39m.\u001b[39;49mapply_masking(action_masks)\n\u001b[1;32m    140\u001b[0m actions \u001b[39m=\u001b[39m distribution\u001b[39m.\u001b[39mget_actions(deterministic\u001b[39m=\u001b[39mdeterministic)\n\u001b[1;32m    141\u001b[0m log_prob \u001b[39m=\u001b[39m distribution\u001b[39m.\u001b[39mlog_prob(actions)\n",
      "File \u001b[0;32m~/anaconda3/envs/surf_rl/lib/python3.9/site-packages/sb3_contrib/common/maskable/distributions.py:159\u001b[0m, in \u001b[0;36mMaskableCategoricalDistribution.apply_masking\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_masking\u001b[39m(\u001b[39mself\u001b[39m, masks: Optional[np\u001b[39m.\u001b[39mndarray]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mMust set distribution parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistribution\u001b[39m.\u001b[39;49mapply_masking(masks)\n",
      "File \u001b[0;32m~/anaconda3/envs/surf_rl/lib/python3.9/site-packages/sb3_contrib/common/maskable/distributions.py:67\u001b[0m, in \u001b[0;36mMaskableCategorical.apply_masking\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m     64\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_logits\n\u001b[1;32m     66\u001b[0m \u001b[39m# Reinitialize with updated logits\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(logits\u001b[39m=\u001b[39;49mlogits)\n\u001b[1;32m     69\u001b[0m \u001b[39m# self.probs may already be cached, so we must force an update\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobs \u001b[39m=\u001b[39m logits_to_probs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits)\n",
      "File \u001b[0;32m~/anaconda3/envs/surf_rl/lib/python3.9/site-packages/torch/distributions/categorical.py:62\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`logits` parameter must be at least one-dimensional.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m     \u001b[39m# Normalize\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits \u001b[39m=\u001b[39m logits \u001b[39m-\u001b[39m logits\u001b[39m.\u001b[39;49mlogsumexp(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobs \u001b[39mif\u001b[39;00m probs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "\n",
    "\n",
    "def mask_fn(env: gym.Env) -> np.ndarray:\n",
    "    # Do whatever you'd like in this function to return the action mask\n",
    "    # for the current env. In this example, we assume the env has a\n",
    "    # helpful method we can rely on.\n",
    "    return env.valid_action_mask()\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    env = GameEnv()\n",
    "    env = ActionMasker(env, mask_fn)  # Wrap to enable masking\n",
    "\n",
    "    # MaskablePPO behaves the same as SB3's PPO unless the env is wrapped\n",
    "    # with ActionMasker. If the wrapper is detected, the masks are automatically\n",
    "    # retrieved and used when learning. Note that MaskablePPO does not accept\n",
    "    # a new action_mask_fn kwarg, as it did in an earlier draft.\n",
    "\n",
    "    # modify network architecture\n",
    "    #policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "    #                        net_arch=dict(pi=[84*2, 84*3, 84, 10], vf=[84*2, 84*3, 84, 10]))\n",
    "\n",
    "\n",
    "    model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=0, tensorboard_log=\"./tensorboards_maskable\")\n",
    "    model.learn(total_timesteps=1_000_000 * 1.5 * 2 * 6)\n",
    "    model.save(f\"models/2048_maskable_{i}\")\n",
    "    evaluate(model, num_steps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 3rd model\n",
    "model = MaskablePPO.load(\"models/2048_maskable_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 5152.48\n",
      "Max score: 14056\n",
      "+-------------------+\n",
      "| 2  | 4  | 64 | 4  |\n",
      "+-------------------+\n",
      "| 8  | 32 |128 |256 |\n",
      "+-------------------+\n",
      "| 4  | 16 | 64 |1024|\n",
      "+-------------------+\n",
      "| 2  | 4  |256 | 2  |\n",
      "+-------------------+\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, num_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a352a48c1b2e4ec895d506ef3207883b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_mask.ppo_mask.MaskablePPO at 0x104f82d30>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GameEnv()\n",
    "env = ActionMasker(env, mask_fn)  # Wrap to enable masking\n",
    "\n",
    "\n",
    "model.set_env(env)\n",
    "model.learn(total_timesteps=1_000_000 * 2 * 2.5, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 5183.496\n",
      "Max score: 15024\n",
      "+-------------------+\n",
      "| 2  | 4  | 64 |128 |\n",
      "+-------------------+\n",
      "| 8  | 16 |256 |1024|\n",
      "+-------------------+\n",
      "| 4  | 32 | 64 |256 |\n",
      "+-------------------+\n",
      "| 2  | 16 | 32 |128 |\n",
      "+-------------------+\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, num_steps=10_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/2048_MaskablePPO_Solved_TT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 4621.532\n",
      "Max score: 13704\n",
      "+-------------------+\n",
      "| 2  | 8  | 64 |128 |\n",
      "+-------------------+\n",
      "| 4  | 16 |128 |256 |\n",
      "+-------------------+\n",
      "| 2  | 8  | 64 |1024|\n",
      "+-------------------+\n",
      "| 8  | 2  | 16 |128 |\n",
      "+-------------------+\n",
      "Average score: 5491.924\n",
      "Max score: 20332\n",
      "+-------------------+\n",
      "| 2  | .  | 2  | .  |\n",
      "+-------------------+\n",
      "| .  | .  | 4  | 4  |\n",
      "+-------------------+\n",
      "| .  | 16 | 32 |2048|\n",
      "+-------------------+\n",
      "| 4  | 4  | 8  | 2  |\n",
      "+-------------------+\n",
      "Average score: 4227.528\n",
      "Max score: 13604\n",
      "+-------------------+\n",
      "| 2  | 4  | 16 |128 |\n",
      "+-------------------+\n",
      "| 4  | 32 | 64 |1024|\n",
      "+-------------------+\n",
      "| 2  | 16 |128 |256 |\n",
      "+-------------------+\n",
      "| 4  | 8  | 32 |128 |\n",
      "+-------------------+\n",
      "Average score: 4677.488\n",
      "Max score: 13848\n",
      "+-------------------+\n",
      "| 2  | 4  | 32 | 64 |\n",
      "+-------------------+\n",
      "| 8  | 32 |256 |1024|\n",
      "+-------------------+\n",
      "| 4  | 8  | 64 |256 |\n",
      "+-------------------+\n",
      "| 2  | 4  | 32 | 64 |\n",
      "+-------------------+\n",
      "Average score: 4886.896\n",
      "Max score: 15800\n",
      "+-------------------+\n",
      "| 2  | 8  |256 | 4  |\n",
      "+-------------------+\n",
      "| 4  | 32 |128 | 16 |\n",
      "+-------------------+\n",
      "| 8  | 4  |1024|512 |\n",
      "+-------------------+\n",
      "| 4  | 2  | 2  | 16 |\n",
      "+-------------------+\n",
      "Average score: 4228.832\n",
      "Max score: 14364\n",
      "+-------------------+\n",
      "| 2  | 8  | 16 |512 |\n",
      "+-------------------+\n",
      "| 8  | 16 |128 |1024|\n",
      "+-------------------+\n",
      "| 4  | 8  | 16 | 4  |\n",
      "+-------------------+\n",
      "| 2  | 32 | 2  | 64 |\n",
      "+-------------------+\n",
      "Average score: 4792.268\n",
      "Max score: 15348\n",
      "+-------------------+\n",
      "| 4  | 2  | 8  | 64 |\n",
      "+-------------------+\n",
      "| 2  | 4  | 64 |512 |\n",
      "+-------------------+\n",
      "| 4  | 32 |128 |1024|\n",
      "+-------------------+\n",
      "| 2  | 8  | 16 |128 |\n",
      "+-------------------+\n",
      "Average score: 4831.26\n",
      "Max score: 15816\n",
      "+-------------------+\n",
      "| 8  | 2  |1024| 2  |\n",
      "+-------------------+\n",
      "| 2  | 16 |256 |512 |\n",
      "+-------------------+\n",
      "| 8  | 32 |128 | 2  |\n",
      "+-------------------+\n",
      "| 2  | 16 | 8  | 16 |\n",
      "+-------------------+\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,9):\n",
    "    model.learn(total_timesteps=1_000_000 * 2 * 2.5)\n",
    "    model.save(f\"models/2048_MaskablePPO_Solved_TT_{i}\")\n",
    "    evaluate(model, num_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_mask.ppo_mask.MaskablePPO at 0x104f82d30>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MaskablePPO in module sb3_contrib.ppo_mask.ppo_mask object:\n",
      "\n",
      "class MaskablePPO(stable_baselines3.common.on_policy_algorithm.OnPolicyAlgorithm)\n",
      " |  MaskablePPO(policy: Union[str, Type[sb3_contrib.common.maskable.policies.MaskableActorCriticPolicy]], env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str], learning_rate: Union[float, Callable[[float], float]] = 0.0003, n_steps: int = 2048, batch_size: Optional[int] = 64, n_epochs: int = 10, gamma: float = 0.99, gae_lambda: float = 0.95, clip_range: Union[float, Callable[[float], float]] = 0.2, clip_range_vf: Union[NoneType, float, Callable[[float], float]] = None, normalize_advantage: bool = True, ent_coef: float = 0.0, vf_coef: float = 0.5, max_grad_norm: float = 0.5, target_kl: Optional[float] = None, stats_window_size: int = 100, tensorboard_log: Optional[str] = None, policy_kwargs: Optional[Dict[str, Any]] = None, verbose: int = 0, seed: Optional[int] = None, device: Union[torch.device, str] = 'auto', _init_setup_model: bool = True)\n",
      " |  \n",
      " |  Proximal Policy Optimization algorithm (PPO) (clip version) with Invalid Action Masking.\n",
      " |  \n",
      " |  Based on the original Stable Baselines 3 implementation.\n",
      " |  \n",
      " |  Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
      " |  Background on Invalid Action Masking: https://arxiv.org/abs/2006.14171\n",
      " |  \n",
      " |  :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
      " |  :param env: The environment to learn from (if registered in Gym, can be str)\n",
      " |  :param learning_rate: The learning rate, it can be a function\n",
      " |      of the current progress remaining (from 1 to 0)\n",
      " |  :param n_steps: The number of steps to run for each environment per update\n",
      " |      (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
      " |  :param batch_size: Minibatch size\n",
      " |  :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
      " |  :param gamma: Discount factor\n",
      " |  :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
      " |  :param clip_range: Clipping parameter, it can be a function of the current progress\n",
      " |      remaining (from 1 to 0).\n",
      " |  :param clip_range_vf: Clipping parameter for the value function,\n",
      " |      it can be a function of the current progress remaining (from 1 to 0).\n",
      " |      This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
      " |      no clipping will be done on the value function.\n",
      " |      IMPORTANT: this clipping depends on the reward scaling.\n",
      " |  :param normalize_advantage: Whether to normalize or not the advantage\n",
      " |  :param ent_coef: Entropy coefficient for the loss calculation\n",
      " |  :param vf_coef: Value function coefficient for the loss calculation\n",
      " |  :param max_grad_norm: The maximum value for the gradient clipping\n",
      " |  :param target_kl: Limit the KL divergence between updates,\n",
      " |      because the clipping is not enough to prevent large update\n",
      " |      see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
      " |      By default, there is no limit on the kl div.\n",
      " |  :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
      " |      the reported success rate, mean episode length, and mean reward over\n",
      " |  :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
      " |  :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
      " |  :param verbose: the verbosity level: 0 no output, 1 info, 2 debug\n",
      " |  :param seed: Seed for the pseudo random generators\n",
      " |  :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
      " |      Setting it to auto, the code will be run on the GPU if possible.\n",
      " |  :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MaskablePPO\n",
      " |      stable_baselines3.common.on_policy_algorithm.OnPolicyAlgorithm\n",
      " |      stable_baselines3.common.base_class.BaseAlgorithm\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, policy: Union[str, Type[sb3_contrib.common.maskable.policies.MaskableActorCriticPolicy]], env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, str], learning_rate: Union[float, Callable[[float], float]] = 0.0003, n_steps: int = 2048, batch_size: Optional[int] = 64, n_epochs: int = 10, gamma: float = 0.99, gae_lambda: float = 0.95, clip_range: Union[float, Callable[[float], float]] = 0.2, clip_range_vf: Union[NoneType, float, Callable[[float], float]] = None, normalize_advantage: bool = True, ent_coef: float = 0.0, vf_coef: float = 0.5, max_grad_norm: float = 0.5, target_kl: Optional[float] = None, stats_window_size: int = 100, tensorboard_log: Optional[str] = None, policy_kwargs: Optional[Dict[str, Any]] = None, verbose: int = 0, seed: Optional[int] = None, device: Union[torch.device, str] = 'auto', _init_setup_model: bool = True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  collect_rollouts(self, env: stable_baselines3.common.vec_env.base_vec_env.VecEnv, callback: stable_baselines3.common.callbacks.BaseCallback, rollout_buffer: stable_baselines3.common.buffers.RolloutBuffer, n_rollout_steps: int, use_masking: bool = True) -> bool\n",
      " |      Collect experiences using the current policy and fill a ``RolloutBuffer``.\n",
      " |      The term rollout here refers to the model-free notion and should not\n",
      " |      be used with the concept of rollout used in model-based RL or planning.\n",
      " |      \n",
      " |      This method is largely identical to the implementation found in the parent class.\n",
      " |      \n",
      " |      :param env: The training environment\n",
      " |      :param callback: Callback that will be called at each step\n",
      " |          (and at the beginning and end of the rollout)\n",
      " |      :param rollout_buffer: Buffer to fill with rollouts\n",
      " |      :param n_steps: Number of experiences to collect per environment\n",
      " |      :param use_masking: Whether or not to use invalid action masks during training\n",
      " |      :return: True if function returned with at least `n_rollout_steps`\n",
      " |          collected, False if callback terminated rollout prematurely.\n",
      " |  \n",
      " |  learn(self: ~SelfMaskablePPO, total_timesteps: int, callback: Union[NoneType, Callable, List[stable_baselines3.common.callbacks.BaseCallback], stable_baselines3.common.callbacks.BaseCallback] = None, log_interval: int = 1, tb_log_name: str = 'PPO', reset_num_timesteps: bool = True, use_masking: bool = True, progress_bar: bool = False) -> ~SelfMaskablePPO\n",
      " |      Return a trained model.\n",
      " |      \n",
      " |      :param total_timesteps: The total number of samples (env steps) to train on\n",
      " |      :param callback: callback(s) called at every step with state of the algorithm.\n",
      " |      :param log_interval: The number of episodes before logging.\n",
      " |      :param tb_log_name: the name of the run for TensorBoard logging\n",
      " |      :param reset_num_timesteps: whether or not to reset the current timestep number (used in logging)\n",
      " |      :param progress_bar: Display a progress bar using tqdm and rich.\n",
      " |      :return: the trained model\n",
      " |  \n",
      " |  predict(self, observation: numpy.ndarray, state: Optional[Tuple[numpy.ndarray, ...]] = None, episode_start: Optional[numpy.ndarray] = None, deterministic: bool = False, action_masks: Optional[numpy.ndarray] = None) -> Tuple[numpy.ndarray, Optional[Tuple[numpy.ndarray, ...]]]\n",
      " |      Get the policy action from an observation (and optional hidden state).\n",
      " |      Includes sugar-coating to handle different observations (e.g. normalizing images).\n",
      " |      \n",
      " |      :param observation: the input observation\n",
      " |      :param state: The last hidden states (can be None, used in recurrent policies)\n",
      " |      :param episode_start: The last masks (can be None, used in recurrent policies)\n",
      " |          this correspond to beginning of episodes,\n",
      " |          where the hidden states of the RNN must be reset.\n",
      " |      :param deterministic: Whether or not to return deterministic actions.\n",
      " |      :return: the model's action and the next hidden state\n",
      " |          (used in recurrent policies)\n",
      " |  \n",
      " |  train(self) -> None\n",
      " |      Update policy using the currently gathered rollout buffer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'policy_aliases': typing.Dict[str, typing.Type[stab...\n",
      " |  \n",
      " |  policy_aliases = {'CnnPolicy': <class 'sb3_contrib.common.maskable.pol...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n",
      " |  \n",
      " |  get_env(self) -> Optional[stable_baselines3.common.vec_env.base_vec_env.VecEnv]\n",
      " |      Returns the current environment (can be None if not defined).\n",
      " |      \n",
      " |      :return: The current environment\n",
      " |  \n",
      " |  get_parameters(self) -> Dict[str, Dict]\n",
      " |      Return the parameters of the agent. This includes parameters from different networks, e.g.\n",
      " |      critics (value functions) and policies (pi functions).\n",
      " |      \n",
      " |      :return: Mapping of from names of the objects to PyTorch state-dicts.\n",
      " |  \n",
      " |  get_vec_normalize_env(self) -> Optional[stable_baselines3.common.vec_env.vec_normalize.VecNormalize]\n",
      " |      Return the ``VecNormalize`` wrapper of the training env\n",
      " |      if it exists.\n",
      " |      \n",
      " |      :return: The ``VecNormalize`` env.\n",
      " |  \n",
      " |  save(self, path: Union[str, pathlib.Path, io.BufferedIOBase], exclude: Optional[Iterable[str]] = None, include: Optional[Iterable[str]] = None) -> None\n",
      " |      Save all the attributes of the object and the model parameters in a zip-file.\n",
      " |      \n",
      " |      :param path: path to the file where the rl agent should be saved\n",
      " |      :param exclude: name of parameters that should be excluded in addition to the default ones\n",
      " |      :param include: name of parameters that might be excluded but should be included anyway\n",
      " |  \n",
      " |  set_env(self, env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv], force_reset: bool = True) -> None\n",
      " |      Checks the validity of the environment, and if it is coherent, set it as the current environment.\n",
      " |      Furthermore wrap any non vectorized env into a vectorized\n",
      " |      checked parameters:\n",
      " |      - observation_space\n",
      " |      - action_space\n",
      " |      \n",
      " |      :param env: The environment for learning a policy\n",
      " |      :param force_reset: Force call to ``reset()`` before training\n",
      " |          to avoid unexpected behavior.\n",
      " |          See issue https://github.com/DLR-RM/stable-baselines3/issues/597\n",
      " |  \n",
      " |  set_logger(self, logger: stable_baselines3.common.logger.Logger) -> None\n",
      " |      Setter for for logger object.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |        When passing a custom logger object,\n",
      " |        this will overwrite ``tensorboard_log`` and ``verbose`` settings\n",
      " |        passed to the constructor.\n",
      " |  \n",
      " |  set_parameters(self, load_path_or_dict: Union[str, Dict[str, torch.Tensor]], exact_match: bool = True, device: Union[torch.device, str] = 'auto') -> None\n",
      " |      Load parameters from a given zip-file or a nested dictionary containing parameters for\n",
      " |      different modules (see ``get_parameters``).\n",
      " |      \n",
      " |      :param load_path_or_iter: Location of the saved data (path or file-like, see ``save``), or a nested\n",
      " |          dictionary containing nn.Module parameters used by the policy. The dictionary maps\n",
      " |          object names to a state-dictionary returned by ``torch.nn.Module.state_dict()``.\n",
      " |      :param exact_match: If True, the given parameters should include parameters for each\n",
      " |          module and each of their parameters, otherwise raises an Exception. If set to False, this\n",
      " |          can be used to update only specific parameters.\n",
      " |      :param device: Device on which the code should run.\n",
      " |  \n",
      " |  set_random_seed(self, seed: Optional[int] = None) -> None\n",
      " |      Set the seed of the pseudo-random generators\n",
      " |      (python, numpy, pytorch, gym, action_space)\n",
      " |      \n",
      " |      :param seed:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n",
      " |  \n",
      " |  load(path: Union[str, pathlib.Path, io.BufferedIOBase], env: Union[gymnasium.core.Env, stable_baselines3.common.vec_env.base_vec_env.VecEnv, NoneType] = None, device: Union[torch.device, str] = 'auto', custom_objects: Optional[Dict[str, Any]] = None, print_system_info: bool = False, force_reset: bool = True, **kwargs) -> ~SelfBaseAlgorithm from abc.ABCMeta\n",
      " |      Load the model from a zip-file.\n",
      " |      Warning: ``load`` re-creates the model from scratch, it does not update it in-place!\n",
      " |      For an in-place load use ``set_parameters`` instead.\n",
      " |      \n",
      " |      :param path: path to the file (or a file-like) where to\n",
      " |          load the agent from\n",
      " |      :param env: the new environment to run the loaded model on\n",
      " |          (can be None if you only need prediction from a trained model) has priority over any saved environment\n",
      " |      :param device: Device on which the code should run.\n",
      " |      :param custom_objects: Dictionary of objects to replace\n",
      " |          upon loading. If a variable is present in this dictionary as a\n",
      " |          key, it will not be deserialized and the corresponding item\n",
      " |          will be used instead. Similar to custom_objects in\n",
      " |          ``keras.models.load_model``. Useful when you have an object in\n",
      " |          file that can not be deserialized.\n",
      " |      :param print_system_info: Whether to print system info from the saved model\n",
      " |          and the current system info (useful to debug loading issues)\n",
      " |      :param force_reset: Force call to ``reset()`` before training\n",
      " |          to avoid unexpected behavior.\n",
      " |          See https://github.com/DLR-RM/stable-baselines3/issues/597\n",
      " |      :param kwargs: extra arguments to change the model when loading\n",
      " |      :return: new model instance with loaded parameters\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n",
      " |  \n",
      " |  logger\n",
      " |      Getter for the logger object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from stable_baselines3.common.base_class.BaseAlgorithm:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "% tensorboard --LOGDIR './tensorboards_maskable'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surf_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
